[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hello world",
    "section": "",
    "text": "Github\n  \n  \n    \n     LinkedIn\n  \n  \n    \n     Email\n  \n\n\n\n\nHello world\nI am a quantitative ecologist who specialises in marine climate-change ecology and climate-smart marine conservation planning. Having worked professionally as a marine ecologist in South Africa, the United Kingdom and Australia, I have diverse interests that revolve around ecological responses to anthropogenic impacts in coastal and oceanic waters.\nThroughout my career, I have worked in large, collaborative groups to facilitate or lead high-impact science in the field of climate-change ecology. In so doing, I have made fundamental contributions to the advancement of:\n\nthe detection and attribution of observed impacts of climate change on marine organisms and ecosystems,\nthe projection of future climate-change risks, and\nthe development of climate-smart systematic conservation planning approaches to mitigate these impacts and risks.\n\nMy expertise in ecological analysis and synthesis culminated in my appointment to several roles in the Fifth and Sixth Assessment Cycle of the Intergovernmental Panel on Climate Change (IPCC). These include serving as a Coordinating Lead Author for the ocean and coastal chapter of the IPCC Sixth Assessment Report (IPCC AR6 Working Group II — 2019–2022), as a Review Editor on the IPCC Special Report on Oceans and Cryosphere in a Changing Climate (SROCC — 2018–2019), as a delegate at Scoping Meetings for the IPCC AR6 (2017) and its Synthesis Report (2019), and as a Contributing Author to several other Chapters and Cross-Chapter Boxes within this cycle. As a result of my contributions, I was selected as a member of the Core Writing and Negotiation Team for the Approval Session of the Summary for Policymakers of the WGII Contribution to the IPCC AR6 (2020–2022).\nOther career highlights include the opportunity to work with a wide range of outstanding HDR students. Although these collaborations have been hugely stimulating, have generated amazing ideas, and have resulted in excellent publications, one less-traditional outcome stands out. Having worked with a PhD candidate at the University of Auckland to publish a high-impact paper (Chaudhary et al. 2021 PNAS), I was approached by award-winning Guardian Australia cartoonist First Dog on the Moon (Andrew Marlton) to develop cartoon based on its results. After several rounds of brainstorming, I was awarded a byline on the work when it was published.\nA final career highlight relates to the intersection between teaching and research. On being appointed as a full-time academic at UniSC in 2013, I was afforded the opportunity to lead the design, accreditation, and delivery of Australia’s first Bachelor of Animal Ecology Degree Program. Completion of this task not only initiated one of the most successful and popular degree programs in the School of Science, Technology and Engineering, but also secured the recruitment of eight new members of staff. This significantly boosted both teaching and research capacity at UniSC and has underpinned the university’s identified research strengths in marine and coastal ecology and in climate-change research. In recognition of my achievements in academic leadership and delivery, I was awarded the Vice-Chancellor and President’s Award for Excellence in Learning and Teaching (2017).\nWhile my research interests remain broad, I continue to work mainly on various aspects of marine climate-change ecology, with a growing focus on potential solutions. Areas of particular interest in this regard include forecasting and projecting future catches for fisheries and further advancing the field of climate-smart systematic conservation planning.\nIn terms of outreach, one of my favourite activities has been my collaboration with Professor Anthony Richardson (UQ/CSIRO) to deliver a series of annual workshops at the University of Queensland, at which we train HDRs, ECRPs and researchers from various universities and State and Federal agencies in the use of the statistical programming environment, R. Having now run for more than a decade, and having included involvement by Associate Professor Chris Brown (University of Tasmania), and Drs Christina Buelow (Griffith University) and Jason Everett (UQ), we have trained more 1,000 scientists in the practice of applied statistics. In so doing, we have built capacity and collaborations that will yield ongoing benefits across the region and nation."
  },
  {
    "objectID": "cv/index.html",
    "href": "cv/index.html",
    "title": "Curriculum vitae",
    "section": "",
    "text": "Download current CV\n  \n\n\n  \n\n\nView the tutorial for this template (+ download link)"
  },
  {
    "objectID": "photography/index.html",
    "href": "photography/index.html",
    "title": "Photography",
    "section": "",
    "text": "As a wizard and scholar of Middle-earth, I have been studying the magic of the natural world for centuries. Through my self-portraits, I aim to capture the essence of my own being and reflect on my own journey through time. Each photograph is a reflection of my own experiences and emotions. Through my photography, I hope to offer a glimpse into my life as a scholar and adventurer, and inspire others to reflect on their own journeys through the world."
  },
  {
    "objectID": "posts/2024-04-19-app-pros-con/index.html",
    "href": "posts/2024-04-19-app-pros-con/index.html",
    "title": "Under development",
    "section": "",
    "text": "Nothing to see here, yet\nOne day, I’ll get to this…."
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "arXiv Preprint | Code\nBuffering…"
  },
  {
    "objectID": "projects/index.html#the-languages-of-middle-earth",
    "href": "projects/index.html#the-languages-of-middle-earth",
    "title": "Projects",
    "section": "",
    "text": "arXiv Preprint | Code\nOver the centuries, I have devoted countless hours to deciphering and translating the ancient scripts and dialects of the various peoples of Middle-earth. My goal is to gain a deeper understanding of the cultures and histories of these peoples by studying their languages. Currently, I am working on a monograph that explores the linguistic roots of the Elvish languages. Through extensive research and analysis, I hope to shed light on the connections between the different dialects of Elvish and their origins. This project has been particularly challenging, as Elvish is a complex and nuanced language, but I am determined to see it through to completion."
  },
  {
    "objectID": "projects/index.html#the-history-of-the-war-of-the-ring",
    "href": "projects/index.html#the-history-of-the-war-of-the-ring",
    "title": "Projects",
    "section": "The History of the War of the Ring",
    "text": "The History of the War of the Ring\n\narXiv Preprint | Code\nI am creating a comprehensive and detailed history of the conflict that goes beyond the surface-level events. By gathering information from a variety of sources, including my own memories, written accounts, and oral histories, I hope to shed new light on this important period in Middle-earth’s history and provide valuable insights into the motivations and actions of the various players involved."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "My Blog",
    "section": "",
    "text": "Converting a calendar export from .ics to .csv\n\n\n\nR programming\n\n\n\nMy very first blog. Ever\n\n\n\nDavid Schoeman\n\n\nSep 27, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/index.html#under-development",
    "href": "projects/index.html#under-development",
    "title": "Projects",
    "section": "",
    "text": "arXiv Preprint | Code\nBuffering…"
  },
  {
    "objectID": "projects/index.html#section",
    "href": "projects/index.html#section",
    "title": "Projects",
    "section": "…",
    "text": "…"
  },
  {
    "objectID": "posts/ics_to_csv/Analysing_calendar_data.html",
    "href": "posts/ics_to_csv/Analysing_calendar_data.html",
    "title": "Converting a calendar export from .ics to .csv",
    "section": "",
    "text": "OK, so this is my first attempt at a blog. It came about because I had a discussion with my Dean about how much time I was spending on teaching vs research vs engagement. Being a data nerd, I record my activities in my Mac Calendar app as a matter of course, so I have a record of what I have been doing. But getting data out of a Calendar app and into a format you can analyse is more difficult. So I decided to write a script to get this done. And because most other options for doing this either cost money, or ask you to trust some or other skeevy website, I thought I’d share my code. Of course, sharing code also means that others can help me improve it, so let me know where I went wrong.\nFirst things first, the packages we’ll need:\n\nlibrary(tidyverse) # For data munging\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(parallel) # To count the cores on my machine, amongst other things\nlibrary(furrr) # To allow me to deploy a function in parallel\n\nLoading required package: future\n\n\nNext, we get the data — after I exported my calendar to a .ics file.\n\ndat &lt;- read_lines(\"/Users/davidschoeman/Dropbox/Documents/USC_Admin/Calendar_export_27092024.ics\") %&gt;% # It's a text file so read it as lines of text\n    tibble() %&gt;% # Make the result into a tibble\n    rename(Info = 1) # It'll have only one column; I'll call it Info\n\nNext, I figure out which rows indicate the start and end of each events, and I clump them together into a tibble:\n\nevents &lt;- tibble(Starts = dat %&gt;%\n                                    pull(Info) %&gt;% \n                                    str_which(\"BEGIN:VEVENT\"), # You might need to change the event-start identifier\n                                 Ends = dat %&gt;%\n                                    pull(Info) %&gt;% \n                                    str_which(\"END:VEVENT\")) # You might need to change the event-end identifier\n\nNext, we need a function to pull out the required details from each event:\n\nget_event_deets &lt;- function(s, e) {\n        d &lt;- dat[s:e,] # A row from dat, based on one row from events\n        get_bit &lt;- function(b) { # A function to look for a keyword and return the data\n            d %&gt;% \n                filter(str_detect(Info, b)) %&gt;% # Find the row in question \n                deframe() %&gt;% # Make it a vector\n                str_remove(b) %&gt;% # Remove the keyword\n                str_sub(2, -1) # Remove just the first character, which is either ; or :, leaving just the info to extract\n            }\n        return(list(Start = get_bit(\"DTSTART\") %&gt;% \n                                str_remove(\"TZID=\") %&gt;% # Remove the time identifier, if present\n                                str_split(\":\") %&gt;% # Split on the colon\n                                unlist() %&gt;% # Make it a vector\n                                nth(2) %&gt;% # Select second element\n                                strptime(., \"%Y%m%dT%H%M%OS\", tz = \"australia/brisbane\"), # Make it time\n                            End = get_bit(\"DTEND\") %&gt;% \n                                str_remove(\"TZID=\") %&gt;% \n                                str_split(\":\") %&gt;% \n                                unlist() %&gt;% \n                                nth(2) %&gt;% \n                                strptime(., \"%Y%m%dT%H%M%OS\", tz = \"australia/brisbane\"), \n                            Event = get_bit(\"SUMMARY\")))\n        }\n\nFinally, we deploy the function against the events tibble…in parallel, because it’s quite large. If you prefer to run on a single core, hash out the lines starting with plan(), and delete future_ from the line with future_pmap:\n\nplan(multisession, workers = detectCores()-4) # Using all but four of my cores\n    csv_cal &lt;- future_pmap(events, # For each row of events\n                           ~get_event_deets(..1, ..2)) %&gt;%  # Get the details\n      bind_rows() %&gt;% # Bind output lists into a tibble\n      arrange(Start) # Place in sequential order\n    plan(sequential)\n\nFinally, we save the output:\n\nwrite_csv(csv_cal, \"/Users/davidschoeman/Dropbox/Documents/USC_Admin/Calendar.csv\") # Write the output to a file\n\nLet’s have a peek at what that tibble looks like (bearing in mind that this is made up data):\n\ncsv_cal\n\n# A tibble: 10,751 × 3\n   Start               End                 Event                                \n   &lt;dttm&gt;              &lt;dttm&gt;              &lt;chr&gt;                                \n 1 2010-02-01 10:00:00 2010-02-01 11:00:00 BEnvSci Review Meet                  \n 2 2012-10-01 11:30:00 2012-10-01 13:00:00 meetings and IT software interests   \n 3 2012-10-02 12:00:00 2012-10-02 13:30:00 New Researchers Group Oct meeting    \n 4 2012-10-10 15:00:00 2012-10-10 17:00:00 Mobile Business Needs Workshop - Aca…\n 5 2012-10-16 12:00:00 2012-10-16 14:00:00 Future Fellowship Applicants Lunch w…\n 6 2012-11-06 09:00:00 2012-11-08 17:00:00 Re: Introductory R Workshop at USC   \n 7 2012-12-18 11:00:00 2012-12-18 12:00:00 EIF Follow Up Meeting                \n 8 2013-01-23 15:00:00 2013-01-23 15:15:00 Dave Shoeman                         \n 9 2013-02-25 15:00:00 2013-02-25 15:30:00 Fraser Island research               \n10 2013-02-26 12:00:00 2013-02-26 13:00:00 ClimateWatch marine science meeting  \n# ℹ 10,741 more rows\n\n\nOK, so now that we have the data, we can search for keywords and see how much time we’ve spent on those activities. We’ll first filter for a specific time period and make sure that our timezone is correctly set (this is important if you pull in the .csv we saved earlier):\n\nstrt_date &lt;- \"2024-01-01\" # A start date\nend_date &lt;- \"2024-12-31\" # An end date\n\ndat &lt;- csv_cal %&gt;% \n        filter(Start &gt;= strt_date & Start &lt;= end_date) %&gt;% # Filter by period of interest\n        mutate(Start = with_tz(Start, \"australia/brisbane\"), # Ensure that my timezone is set correctly\n                     End = with_tz(End, \"australia/brisbane\"), # And again\n                     Duration = (End - Start) %&gt;% \n                        as.numeric()/60/60) # Compute the duration of each event in hours\n\nNext, we group by event name, sum the time committed and remove any “activities” that might be placeholders rather than actual time commitments:\n\ntime_spent &lt;- dat %&gt;% \n        group_by(Event) %&gt;% \n        summarise(Time = sum(Duration)) %&gt;% # Compute the summed duration committed to each event\n        filter(str_detect(Event, \"WFH|Playing Wordle\", negate = TRUE)) # Remove any specific events that are not for logging time\n\nFinally, we write a short function to extract all lines containing a specific keyword, then use that function to look around:\n\nget_time &lt;- function(a) {\n            ts &lt;- time_spent %&gt;% \n                filter(str_detect(Event, a))\n            return(list(Time_table = data.frame(ts), # Output all rows containing the keyword, in case you need to check\n                         Total = ts %&gt;% \n                                        pull(Time) %&gt;% \n                                        sum())) # Also report total time spent on events containing that keyword\n            }\n\nOK, so let’s see how much time I have spent on my year-2 stats course so far this year:\n\nget_time(\"ANM203\")$Total\n\n[1] 239.75\n\n\nSooo…240 hours…and we still have four weeks of semester to go, and a major assignment to mark…oh well…\nOK, in the end, you can see that it’s not particularly difficult, but I hope this saves you some time if you ever want to analyse calendar data yourself."
  },
  {
    "objectID": "posts/ics_to_csv/index.html",
    "href": "posts/ics_to_csv/index.html",
    "title": "Converting a calendar export from .ics to .csv",
    "section": "",
    "text": "OK, so this is my first attempt at a blog. It came about because I had a discussion with my Dean about how much time I was spending on teaching vs research vs engagement. Being a data nerd, I record my activities in my Mac Calendar app as a matter of course, so I have a record of what I have been doing. But getting data out of a Calendar app and into a format you can analyse is more difficult. So I decided to write a script to get this done. And because most other options for doing this either cost money, or ask you to trust some or other skeevy website, I thought I’d share my code. Of course, sharing code also means that others can help me improve it, so let me know where I went wrong.\nFirst things first, the packages we’ll need:\n\nlibrary(tidyverse) # For data munging\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(parallel) # To count the cores on my machine, amongst other things\nlibrary(furrr) # To allow me to deploy a function in parallel\n\nLoading required package: future\n\n\nNext, we get the data — after I exported my calendar to a .ics file.\n\ndat &lt;- read_lines(\"/Users/davidschoeman/Dropbox/Documents/USC_Admin/Calendar_export_27092024.ics\") %&gt;% # It's a text file so read it as lines of text\n    tibble() %&gt;% # Make the result into a tibble\n    rename(Info = 1) # It'll have only one column; I'll call it Info\n\nNext, I figure out which rows indicate the start and end of each events, and I clump them together into a tibble:\n\nevents &lt;- tibble(Starts = dat %&gt;%\n                                    pull(Info) %&gt;% \n                                    str_which(\"BEGIN:VEVENT\"), # You might need to change the event-start identifier\n                                 Ends = dat %&gt;%\n                                    pull(Info) %&gt;% \n                                    str_which(\"END:VEVENT\")) # You might need to change the event-end identifier\n\nNext, we need a function to pull out the required details from each event:\n\nget_event_deets &lt;- function(s, e) {\n        d &lt;- dat[s:e,] # A row from dat, based on one row from events\n        get_bit &lt;- function(b) { # A function to look for a keyword and return the data\n            d %&gt;% \n                filter(str_detect(Info, b)) %&gt;% # Find the row in question \n                deframe() %&gt;% # Make it a vector\n                str_remove(b) %&gt;% # Remove the keyword\n                str_sub(2, -1) # Remove just the first character, which is either ; or :, leaving just the info to extract\n            }\n        return(list(Start = get_bit(\"DTSTART\") %&gt;% \n                                str_remove(\"TZID=\") %&gt;% # Remove the time identifier, if present\n                                str_split(\":\") %&gt;% # Split on the colon\n                                unlist() %&gt;% # Make it a vector\n                                nth(2) %&gt;% # Select second element\n                                strptime(., \"%Y%m%dT%H%M%OS\", tz = \"australia/brisbane\"), # Make it time\n                            End = get_bit(\"DTEND\") %&gt;% \n                                str_remove(\"TZID=\") %&gt;% \n                                str_split(\":\") %&gt;% \n                                unlist() %&gt;% \n                                nth(2) %&gt;% \n                                strptime(., \"%Y%m%dT%H%M%OS\", tz = \"australia/brisbane\"), \n                            Event = get_bit(\"SUMMARY\")))\n        }\n\nFinally, we deploy the function against the events tibble…in parallel, because it’s quite large. If you prefer to run on a single core, hash out the lines starting with plan(), and delete future_ from the line with future_pmap:\n\nplan(multisession, workers = detectCores()-4) # Using all but four of my cores\n    csv_cal &lt;- future_pmap(events, # For each row of events\n                           ~get_event_deets(..1, ..2)) %&gt;%  # Get the details\n      bind_rows() %&gt;% # Bind output lists into a tibble\n      arrange(Start) # Place in sequential order\n    plan(sequential)\n\nFinally, we save the output:\n\nwrite_csv(csv_cal, \"/Users/davidschoeman/Dropbox/Documents/USC_Admin/Calendar.csv\") # Write the output to a file\n\nLet’s have a peek at what that tibble looks like (bearing in mind that this is made up data):\n\ncsv_cal\n\n# A tibble: 10,751 × 3\n   Start               End                 Event                                \n   &lt;dttm&gt;              &lt;dttm&gt;              &lt;chr&gt;                                \n 1 2010-02-01 10:00:00 2010-02-01 11:00:00 BEnvSci Review Meet                  \n 2 2012-10-01 11:30:00 2012-10-01 13:00:00 meetings and IT software interests   \n 3 2012-10-02 12:00:00 2012-10-02 13:30:00 New Researchers Group Oct meeting    \n 4 2012-10-10 15:00:00 2012-10-10 17:00:00 Mobile Business Needs Workshop - Aca…\n 5 2012-10-16 12:00:00 2012-10-16 14:00:00 Future Fellowship Applicants Lunch w…\n 6 2012-11-06 09:00:00 2012-11-08 17:00:00 Re: Introductory R Workshop at USC   \n 7 2012-12-18 11:00:00 2012-12-18 12:00:00 EIF Follow Up Meeting                \n 8 2013-01-23 15:00:00 2013-01-23 15:15:00 Dave Shoeman                         \n 9 2013-02-25 15:00:00 2013-02-25 15:30:00 Fraser Island research               \n10 2013-02-26 12:00:00 2013-02-26 13:00:00 ClimateWatch marine science meeting  \n# ℹ 10,741 more rows\n\n\nOK, so now that we have the data, we can search for keywords and see how much time we’ve spent on those activities. We’ll first filter for a specific time period and make sure that our timezone is correctly set (this is important if you pull in the .csv we saved earlier):\n\nstrt_date &lt;- \"2024-01-01\" # A start date\nend_date &lt;- \"2024-12-31\" # An end date\n\ndat &lt;- csv_cal %&gt;% \n        filter(Start &gt;= strt_date & Start &lt;= end_date) %&gt;% # Filter by period of interest\n        mutate(Start = with_tz(Start, \"australia/brisbane\"), # Ensure that my timezone is set correctly\n                     End = with_tz(End, \"australia/brisbane\"), # And again\n                     Duration = (End - Start) %&gt;% \n                        as.numeric()/60/60) # Compute the duration of each event in hours\n\nNext, we group by event name, sum the time committed and remove any “activities” that might be placeholders rather than actual time commitments:\n\ntime_spent &lt;- dat %&gt;% \n        group_by(Event) %&gt;% \n        summarise(Time = sum(Duration)) %&gt;% # Compute the summed duration committed to each event\n        filter(str_detect(Event, \"WFH|Playing Wordle\", negate = TRUE)) # Remove any specific events that are not for logging time\n\nFinally, we write a short function to extract all lines containing a specific keyword, then use that function to look around:\n\nget_time &lt;- function(a) {\n            ts &lt;- time_spent %&gt;% \n                filter(str_detect(Event, a))\n            return(list(Time_table = data.frame(ts), # Output all rows containing the keyword, in case you need to check\n                         Total = ts %&gt;% \n                                        pull(Time) %&gt;% \n                                        sum())) # Also report total time spent on events containing that keyword\n            }\n\nOK, so let’s see how much time I have spent on my year-2 stats course so far this year:\n\nget_time(\"ANM203\")$Total\n\n[1] 239.75\n\n\nSooo…240 hours…and we still have four weeks of semester to go, and a major assignment to mark…oh well…\nOK, in the end, you can see that it’s not particularly difficult, but I hope this saves you some time if you ever want to analyse calendar data yourself."
  },
  {
    "objectID": "posts/2024-09-27_ics_to_csv/index.html",
    "href": "posts/2024-09-27_ics_to_csv/index.html",
    "title": "Converting a calendar export from .ics to .csv",
    "section": "",
    "text": "OK, so this is my first attempt at a blog. It came about because I had a discussion with my Dean about how much time I was spending on teaching vs research vs engagement. Being a data nerd, I record my activities in my Mac Calendar app as a matter of course, so I have a record of what I have been doing. But getting data out of a Calendar app and into a format you can analyse is more difficult. So I decided to write a script to get this done. And because most other options for doing this either cost money, or ask you to trust some or other skeevy website, I thought I’d share my code. Of course, sharing code also means that others can help me improve it, so let me know where I went wrong.\nFirst things first, the packages we’ll need:\n\nlibrary(tidyverse) # For data munging\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(parallel) # To count the cores on my machine, amongst other things\nlibrary(furrr) # To allow me to deploy a function in parallel\n\nLoading required package: future\n\n\nNext, we get the data — after I exported my calendar to a .ics file.\n\ndat &lt;- read_lines(\"/Users/davidschoeman/Dropbox/Documents/USC_Admin/Calendar_export_27092024.ics\") %&gt;% # It's a text file so read it as lines of text\n    tibble() %&gt;% # Make the result into a tibble\n    rename(Info = 1) # It'll have only one column; I'll call it Info\n\nNext, I figure out which rows indicate the start and end of each events, and I clump them together into a tibble:\n\nevents &lt;- tibble(Starts = dat %&gt;%\n                                    pull(Info) %&gt;% \n                                    str_which(\"BEGIN:VEVENT\"), # You might need to change the event-start identifier\n                                 Ends = dat %&gt;%\n                                    pull(Info) %&gt;% \n                                    str_which(\"END:VEVENT\")) # You might need to change the event-end identifier\n\nNext, we need a function to pull out the required details from each event:\n\nget_event_deets &lt;- function(s, e) {\n        d &lt;- dat[s:e,] # A row from dat, based on one row from events\n        get_bit &lt;- function(b) { # A function to look for a keyword and return the data\n            d %&gt;% \n                filter(str_detect(Info, b)) %&gt;% # Find the row in question \n                deframe() %&gt;% # Make it a vector\n                str_remove(b) %&gt;% # Remove the keyword\n                str_sub(2, -1) # Remove just the first character, which is either ; or :, leaving just the info to extract\n            }\n        return(list(Start = get_bit(\"DTSTART\") %&gt;% \n                                str_remove(\"TZID=\") %&gt;% # Remove the time identifier, if present\n                                str_split(\":\") %&gt;% # Split on the colon\n                                unlist() %&gt;% # Make it a vector\n                                nth(2) %&gt;% # Select second element\n                                strptime(., \"%Y%m%dT%H%M%OS\", tz = \"australia/brisbane\"), # Make it time\n                            End = get_bit(\"DTEND\") %&gt;% \n                                str_remove(\"TZID=\") %&gt;% \n                                str_split(\":\") %&gt;% \n                                unlist() %&gt;% \n                                nth(2) %&gt;% \n                                strptime(., \"%Y%m%dT%H%M%OS\", tz = \"australia/brisbane\"), \n                            Event = get_bit(\"SUMMARY\")))\n        }\n\nFinally, we deploy the function against the events tibble…in parallel, because it’s quite large. If you prefer to run on a single core, hash out the lines starting with plan(), and delete future_ from the line with future_pmap:\n\nplan(multisession, workers = detectCores()-4) # Using all but four of my cores\n    csv_cal &lt;- future_pmap(events, # For each row of events\n                           ~get_event_deets(..1, ..2)) %&gt;%  # Get the details\n      bind_rows() %&gt;% # Bind output lists into a tibble\n      arrange(Start) # Place in sequential order\n    plan(sequential)\n\nFinally, we save the output:\n\nwrite_csv(csv_cal, \"/Users/davidschoeman/Dropbox/Documents/USC_Admin/Calendar.csv\") # Write the output to a file\n\nLet’s have a peek at what that tibble looks like (bearing in mind that this is made up data):\n\ncsv_cal\n\n# A tibble: 10,751 × 3\n   Start               End                 Event                                \n   &lt;dttm&gt;              &lt;dttm&gt;              &lt;chr&gt;                                \n 1 2010-02-01 10:00:00 2010-02-01 11:00:00 BEnvSci Review Meet                  \n 2 2012-10-01 11:30:00 2012-10-01 13:00:00 meetings and IT software interests   \n 3 2012-10-02 12:00:00 2012-10-02 13:30:00 New Researchers Group Oct meeting    \n 4 2012-10-10 15:00:00 2012-10-10 17:00:00 Mobile Business Needs Workshop - Aca…\n 5 2012-10-16 12:00:00 2012-10-16 14:00:00 Future Fellowship Applicants Lunch w…\n 6 2012-11-06 09:00:00 2012-11-08 17:00:00 Re: Introductory R Workshop at USC   \n 7 2012-12-18 11:00:00 2012-12-18 12:00:00 EIF Follow Up Meeting                \n 8 2013-01-23 15:00:00 2013-01-23 15:15:00 Dave Shoeman                         \n 9 2013-02-25 15:00:00 2013-02-25 15:30:00 Fraser Island research               \n10 2013-02-26 12:00:00 2013-02-26 13:00:00 ClimateWatch marine science meeting  \n# ℹ 10,741 more rows\n\n\nOK, so now that we have the data, we can search for keywords and see how much time we’ve spent on those activities. We’ll first filter for a specific time period and make sure that our timezone is correctly set (this is important if you pull in the .csv we saved earlier):\n\nstrt_date &lt;- \"2024-01-01\" # A start date\nend_date &lt;- \"2024-12-31\" # An end date\n\ndat &lt;- csv_cal %&gt;% \n        filter(Start &gt;= strt_date & Start &lt;= end_date) %&gt;% # Filter by period of interest\n        mutate(Start = with_tz(Start, \"australia/brisbane\"), # Ensure that my timezone is set correctly\n                     End = with_tz(End, \"australia/brisbane\"), # And again\n                     Duration = (End - Start) %&gt;% \n                        as.numeric()/60/60) # Compute the duration of each event in hours\n\nNext, we group by event name, sum the time committed and remove any “activities” that might be placeholders rather than actual time commitments:\n\ntime_spent &lt;- dat %&gt;% \n        group_by(Event) %&gt;% \n        summarise(Time = sum(Duration)) %&gt;% # Compute the summed duration committed to each event\n        filter(str_detect(Event, \"WFH|Playing Wordle\", negate = TRUE)) # Remove any specific events that are not for logging time\n\nFinally, we write a short function to extract all lines containing a specific keyword, then use that function to look around:\n\nget_time &lt;- function(a) {\n            ts &lt;- time_spent %&gt;% \n                filter(str_detect(Event, a))\n            return(list(Time_table = data.frame(ts), # Output all rows containing the keyword, in case you need to check\n                         Total = ts %&gt;% \n                                        pull(Time) %&gt;% \n                                        sum())) # Also report total time spent on events containing that keyword\n            }\n\nOK, so let’s see how much time I have spent on my year-2 stats course so far this year:\n\nget_time(\"ANM203\")$Total\n\n[1] 239.75\n\n\nSooo…240 hours…and we still have four weeks of semester to go, and a major assignment to mark…oh well…\nOK, in the end, you can see that it’s not particularly difficult, but I hope this saves you some time if you ever want to analyse calendar data yourself."
  }
]