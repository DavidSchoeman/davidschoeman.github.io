[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hello world",
    "section": "",
    "text": "Github\n  \n  \n    \n     LinkedIn\n  \n  \n    \n     Email\n  \n\n\n\n\nHello world\nI am a quantitative ecologist who specialises in marine climate-change ecology and climate-smart marine conservation planning. Having worked professionally as a marine ecologist in South Africa, the United Kingdom and Australia, I have diverse interests that revolve around ecological responses to anthropogenic impacts in coastal and oceanic waters.\nThroughout my career, I have worked in large, collaborative groups to facilitate or lead high-impact science in the field of climate-change ecology. In so doing, I have made fundamental contributions to the advancement of:\n\nthe detection and attribution of observed impacts of climate change on marine organisms and ecosystems,\nthe projection of future climate-change risks, and\nthe development of climate-smart systematic conservation planning approaches to mitigate these impacts and risks.\n\nMy expertise in ecological analysis and synthesis culminated in my appointment to several roles in the Fifth and Sixth Assessment Cycle of the Intergovernmental Panel on Climate Change (IPCC). These include serving as a Coordinating Lead Author for the ocean and coastal chapter of the IPCC Sixth Assessment Report (IPCC AR6 Working Group II — 2019–2022), as a Review Editor on the IPCC Special Report on Oceans and Cryosphere in a Changing Climate (SROCC — 2018–2019), as a delegate at Scoping Meetings for the IPCC AR6 (2017) and its Synthesis Report (2019), and as a Contributing Author to several other Chapters and Cross-Chapter Boxes within this cycle. As a result of my contributions, I was selected as a member of the Core Writing and Negotiation Team for the Approval Session of the Summary for Policymakers of the WGII Contribution to the IPCC AR6 (2020–2022).\nOther career highlights include the opportunity to work with a wide range of outstanding HDR students. Although these collaborations have been hugely stimulating, have generated amazing ideas, and have resulted in excellent publications, one less-traditional outcome stands out. Having worked with a PhD candidate at the University of Auckland to publish a high-impact paper (Chaudhary et al. 2021 PNAS), I was approached by award-winning Guardian Australia cartoonist First Dog on the Moon (Andrew Marlton) to develop cartoon based on its results. After several rounds of brainstorming, I was awarded a byline on the work when it was published.\nA final career highlight relates to the intersection between teaching and research. On being appointed as a full-time academic at UniSC in 2013, I was afforded the opportunity to lead the design, accreditation, and delivery of Australia’s first Bachelor of Animal Ecology Degree Program. Completion of this task not only initiated one of the most successful and popular degree programs in the School of Science, Technology and Engineering, but also secured the recruitment of eight new members of staff. This significantly boosted both teaching and research capacity at UniSC and has underpinned the university’s identified research strengths in marine and coastal ecology and in climate-change research. In recognition of my achievements in academic leadership and delivery, I was awarded the Vice-Chancellor and President’s Award for Excellence in Learning and Teaching (2017).\nWhile my research interests remain broad, I continue to work mainly on various aspects of marine climate-change ecology, with a growing focus on potential solutions. Areas of particular interest in this regard include forecasting and projecting future catches for fisheries and further advancing the field of climate-smart systematic conservation planning.\nIn terms of outreach, one of my favourite activities has been my collaboration with Professor Anthony Richardson (UQ/CSIRO) to deliver a series of annual workshops at the University of Queensland, at which we train HDRs, ECRPs and researchers from various universities and State and Federal agencies in the use of the statistical programming environment, R. Having now run for more than a decade, and having included involvement by Associate Professor Chris Brown (University of Tasmania), and Drs Christina Buelow (Griffith University) and Jason Everett (UQ), we have trained more 1,000 scientists in the practice of applied statistics. In so doing, we have built capacity and collaborations that will yield ongoing benefits across the region and nation."
  },
  {
    "objectID": "cv/index.html",
    "href": "cv/index.html",
    "title": "Curriculum vitae",
    "section": "",
    "text": "Download current CV\n  \n\n\n  \n\n\nView the tutorial for this template (+ download link)"
  },
  {
    "objectID": "photography/index.html",
    "href": "photography/index.html",
    "title": "Photography",
    "section": "",
    "text": "As a wizard and scholar of Middle-earth, I have been studying the magic of the natural world for centuries. Through my self-portraits, I aim to capture the essence of my own being and reflect on my own journey through time. Each photograph is a reflection of my own experiences and emotions. Through my photography, I hope to offer a glimpse into my life as a scholar and adventurer, and inspire others to reflect on their own journeys through the world."
  },
  {
    "objectID": "posts/2024-04-19-app-pros-con/index.html",
    "href": "posts/2024-04-19-app-pros-con/index.html",
    "title": "Under development",
    "section": "",
    "text": "Nothing to see here, yet\nOne day, I’ll get to this…."
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "arXiv Preprint | Code\nBuffering…"
  },
  {
    "objectID": "projects/index.html#the-languages-of-middle-earth",
    "href": "projects/index.html#the-languages-of-middle-earth",
    "title": "Projects",
    "section": "",
    "text": "arXiv Preprint | Code\nOver the centuries, I have devoted countless hours to deciphering and translating the ancient scripts and dialects of the various peoples of Middle-earth. My goal is to gain a deeper understanding of the cultures and histories of these peoples by studying their languages. Currently, I am working on a monograph that explores the linguistic roots of the Elvish languages. Through extensive research and analysis, I hope to shed light on the connections between the different dialects of Elvish and their origins. This project has been particularly challenging, as Elvish is a complex and nuanced language, but I am determined to see it through to completion."
  },
  {
    "objectID": "projects/index.html#the-history-of-the-war-of-the-ring",
    "href": "projects/index.html#the-history-of-the-war-of-the-ring",
    "title": "Projects",
    "section": "The History of the War of the Ring",
    "text": "The History of the War of the Ring\n\narXiv Preprint | Code\nI am creating a comprehensive and detailed history of the conflict that goes beyond the surface-level events. By gathering information from a variety of sources, including my own memories, written accounts, and oral histories, I hope to shed new light on this important period in Middle-earth’s history and provide valuable insights into the motivations and actions of the various players involved."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "My Blog",
    "section": "",
    "text": "Explorations in functional programming in R\n\n\n\nR programming\n\n\n\nWriting a complex function with unlimited arguments and masked variables.\n\n\n\nDavid Schoeman\n\n\nOct 10, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConverting a calendar export from .ics to .csv\n\n\n\nR programming\n\n\n\nMy very first blog. Ever.\n\n\n\nDavid Schoeman\n\n\nSep 27, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/index.html#under-development",
    "href": "projects/index.html#under-development",
    "title": "Projects",
    "section": "",
    "text": "arXiv Preprint | Code\nBuffering…"
  },
  {
    "objectID": "projects/index.html#section",
    "href": "projects/index.html#section",
    "title": "Projects",
    "section": "…",
    "text": "…"
  },
  {
    "objectID": "posts/ics_to_csv/Analysing_calendar_data.html",
    "href": "posts/ics_to_csv/Analysing_calendar_data.html",
    "title": "Converting a calendar export from .ics to .csv",
    "section": "",
    "text": "OK, so this is my first attempt at a blog. It came about because I had a discussion with my Dean about how much time I was spending on teaching vs research vs engagement. Being a data nerd, I record my activities in my Mac Calendar app as a matter of course, so I have a record of what I have been doing. But getting data out of a Calendar app and into a format you can analyse is more difficult. So I decided to write a script to get this done. And because most other options for doing this either cost money, or ask you to trust some or other skeevy website, I thought I’d share my code. Of course, sharing code also means that others can help me improve it, so let me know where I went wrong.\nFirst things first, the packages we’ll need:\n\nlibrary(tidyverse) # For data munging\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(parallel) # To count the cores on my machine, amongst other things\nlibrary(furrr) # To allow me to deploy a function in parallel\n\nLoading required package: future\n\n\nNext, we get the data — after I exported my calendar to a .ics file.\n\ndat &lt;- read_lines(\"/Users/davidschoeman/Dropbox/Documents/USC_Admin/Calendar_export_27092024.ics\") %&gt;% # It's a text file so read it as lines of text\n    tibble() %&gt;% # Make the result into a tibble\n    rename(Info = 1) # It'll have only one column; I'll call it Info\n\nNext, I figure out which rows indicate the start and end of each events, and I clump them together into a tibble:\n\nevents &lt;- tibble(Starts = dat %&gt;%\n                                    pull(Info) %&gt;% \n                                    str_which(\"BEGIN:VEVENT\"), # You might need to change the event-start identifier\n                                 Ends = dat %&gt;%\n                                    pull(Info) %&gt;% \n                                    str_which(\"END:VEVENT\")) # You might need to change the event-end identifier\n\nNext, we need a function to pull out the required details from each event:\n\nget_event_deets &lt;- function(s, e) {\n        d &lt;- dat[s:e,] # A row from dat, based on one row from events\n        get_bit &lt;- function(b) { # A function to look for a keyword and return the data\n            d %&gt;% \n                filter(str_detect(Info, b)) %&gt;% # Find the row in question \n                deframe() %&gt;% # Make it a vector\n                str_remove(b) %&gt;% # Remove the keyword\n                str_sub(2, -1) # Remove just the first character, which is either ; or :, leaving just the info to extract\n            }\n        return(list(Start = get_bit(\"DTSTART\") %&gt;% \n                                str_remove(\"TZID=\") %&gt;% # Remove the time identifier, if present\n                                str_split(\":\") %&gt;% # Split on the colon\n                                unlist() %&gt;% # Make it a vector\n                                nth(2) %&gt;% # Select second element\n                                strptime(., \"%Y%m%dT%H%M%OS\", tz = \"australia/brisbane\"), # Make it time\n                            End = get_bit(\"DTEND\") %&gt;% \n                                str_remove(\"TZID=\") %&gt;% \n                                str_split(\":\") %&gt;% \n                                unlist() %&gt;% \n                                nth(2) %&gt;% \n                                strptime(., \"%Y%m%dT%H%M%OS\", tz = \"australia/brisbane\"), \n                            Event = get_bit(\"SUMMARY\")))\n        }\n\nFinally, we deploy the function against the events tibble…in parallel, because it’s quite large. If you prefer to run on a single core, hash out the lines starting with plan(), and delete future_ from the line with future_pmap:\n\nplan(multisession, workers = detectCores()-4) # Using all but four of my cores\n    csv_cal &lt;- future_pmap(events, # For each row of events\n                           ~get_event_deets(..1, ..2)) %&gt;%  # Get the details\n      bind_rows() %&gt;% # Bind output lists into a tibble\n      arrange(Start) # Place in sequential order\n    plan(sequential)\n\nFinally, we save the output:\n\nwrite_csv(csv_cal, \"/Users/davidschoeman/Dropbox/Documents/USC_Admin/Calendar.csv\") # Write the output to a file\n\nLet’s have a peek at what that tibble looks like (bearing in mind that this is made up data):\n\ncsv_cal\n\n# A tibble: 10,751 × 3\n   Start               End                 Event                                \n   &lt;dttm&gt;              &lt;dttm&gt;              &lt;chr&gt;                                \n 1 2010-02-01 10:00:00 2010-02-01 11:00:00 BEnvSci Review Meet                  \n 2 2012-10-01 11:30:00 2012-10-01 13:00:00 meetings and IT software interests   \n 3 2012-10-02 12:00:00 2012-10-02 13:30:00 New Researchers Group Oct meeting    \n 4 2012-10-10 15:00:00 2012-10-10 17:00:00 Mobile Business Needs Workshop - Aca…\n 5 2012-10-16 12:00:00 2012-10-16 14:00:00 Future Fellowship Applicants Lunch w…\n 6 2012-11-06 09:00:00 2012-11-08 17:00:00 Re: Introductory R Workshop at USC   \n 7 2012-12-18 11:00:00 2012-12-18 12:00:00 EIF Follow Up Meeting                \n 8 2013-01-23 15:00:00 2013-01-23 15:15:00 Dave Shoeman                         \n 9 2013-02-25 15:00:00 2013-02-25 15:30:00 Fraser Island research               \n10 2013-02-26 12:00:00 2013-02-26 13:00:00 ClimateWatch marine science meeting  \n# ℹ 10,741 more rows\n\n\nOK, so now that we have the data, we can search for keywords and see how much time we’ve spent on those activities. We’ll first filter for a specific time period and make sure that our timezone is correctly set (this is important if you pull in the .csv we saved earlier):\n\nstrt_date &lt;- \"2024-01-01\" # A start date\nend_date &lt;- \"2024-12-31\" # An end date\n\ndat &lt;- csv_cal %&gt;% \n        filter(Start &gt;= strt_date & Start &lt;= end_date) %&gt;% # Filter by period of interest\n        mutate(Start = with_tz(Start, \"australia/brisbane\"), # Ensure that my timezone is set correctly\n                     End = with_tz(End, \"australia/brisbane\"), # And again\n                     Duration = (End - Start) %&gt;% \n                        as.numeric()/60/60) # Compute the duration of each event in hours\n\nNext, we group by event name, sum the time committed and remove any “activities” that might be placeholders rather than actual time commitments:\n\ntime_spent &lt;- dat %&gt;% \n        group_by(Event) %&gt;% \n        summarise(Time = sum(Duration)) %&gt;% # Compute the summed duration committed to each event\n        filter(str_detect(Event, \"WFH|Playing Wordle\", negate = TRUE)) # Remove any specific events that are not for logging time\n\nFinally, we write a short function to extract all lines containing a specific keyword, then use that function to look around:\n\nget_time &lt;- function(a) {\n            ts &lt;- time_spent %&gt;% \n                filter(str_detect(Event, a))\n            return(list(Time_table = data.frame(ts), # Output all rows containing the keyword, in case you need to check\n                         Total = ts %&gt;% \n                                        pull(Time) %&gt;% \n                                        sum())) # Also report total time spent on events containing that keyword\n            }\n\nOK, so let’s see how much time I have spent on my year-2 stats course so far this year:\n\nget_time(\"ANM203\")$Total\n\n[1] 239.75\n\n\nSooo…240 hours…and we still have four weeks of semester to go, and a major assignment to mark…oh well…\nOK, in the end, you can see that it’s not particularly difficult, but I hope this saves you some time if you ever want to analyse calendar data yourself."
  },
  {
    "objectID": "posts/ics_to_csv/index.html",
    "href": "posts/ics_to_csv/index.html",
    "title": "Converting a calendar export from .ics to .csv",
    "section": "",
    "text": "OK, so this is my first attempt at a blog. It came about because I had a discussion with my Dean about how much time I was spending on teaching vs research vs engagement. Being a data nerd, I record my activities in my Mac Calendar app as a matter of course, so I have a record of what I have been doing. But getting data out of a Calendar app and into a format you can analyse is more difficult. So I decided to write a script to get this done. And because most other options for doing this either cost money, or ask you to trust some or other skeevy website, I thought I’d share my code. Of course, sharing code also means that others can help me improve it, so let me know where I went wrong.\nFirst things first, the packages we’ll need:\n\nlibrary(tidyverse) # For data munging\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(parallel) # To count the cores on my machine, amongst other things\nlibrary(furrr) # To allow me to deploy a function in parallel\n\nLoading required package: future\n\n\nNext, we get the data — after I exported my calendar to a .ics file.\n\ndat &lt;- read_lines(\"/Users/davidschoeman/Dropbox/Documents/USC_Admin/Calendar_export_27092024.ics\") %&gt;% # It's a text file so read it as lines of text\n    tibble() %&gt;% # Make the result into a tibble\n    rename(Info = 1) # It'll have only one column; I'll call it Info\n\nNext, I figure out which rows indicate the start and end of each events, and I clump them together into a tibble:\n\nevents &lt;- tibble(Starts = dat %&gt;%\n                                    pull(Info) %&gt;% \n                                    str_which(\"BEGIN:VEVENT\"), # You might need to change the event-start identifier\n                                 Ends = dat %&gt;%\n                                    pull(Info) %&gt;% \n                                    str_which(\"END:VEVENT\")) # You might need to change the event-end identifier\n\nNext, we need a function to pull out the required details from each event:\n\nget_event_deets &lt;- function(s, e) {\n        d &lt;- dat[s:e,] # A row from dat, based on one row from events\n        get_bit &lt;- function(b) { # A function to look for a keyword and return the data\n            d %&gt;% \n                filter(str_detect(Info, b)) %&gt;% # Find the row in question \n                deframe() %&gt;% # Make it a vector\n                str_remove(b) %&gt;% # Remove the keyword\n                str_sub(2, -1) # Remove just the first character, which is either ; or :, leaving just the info to extract\n            }\n        return(list(Start = get_bit(\"DTSTART\") %&gt;% \n                                str_remove(\"TZID=\") %&gt;% # Remove the time identifier, if present\n                                str_split(\":\") %&gt;% # Split on the colon\n                                unlist() %&gt;% # Make it a vector\n                                nth(2) %&gt;% # Select second element\n                                strptime(., \"%Y%m%dT%H%M%OS\", tz = \"australia/brisbane\"), # Make it time\n                            End = get_bit(\"DTEND\") %&gt;% \n                                str_remove(\"TZID=\") %&gt;% \n                                str_split(\":\") %&gt;% \n                                unlist() %&gt;% \n                                nth(2) %&gt;% \n                                strptime(., \"%Y%m%dT%H%M%OS\", tz = \"australia/brisbane\"), \n                            Event = get_bit(\"SUMMARY\")))\n        }\n\nFinally, we deploy the function against the events tibble…in parallel, because it’s quite large. If you prefer to run on a single core, hash out the lines starting with plan(), and delete future_ from the line with future_pmap:\n\nplan(multisession, workers = detectCores()-4) # Using all but four of my cores\n    csv_cal &lt;- future_pmap(events, # For each row of events\n                           ~get_event_deets(..1, ..2)) %&gt;%  # Get the details\n      bind_rows() %&gt;% # Bind output lists into a tibble\n      arrange(Start) # Place in sequential order\n    plan(sequential)\n\nFinally, we save the output:\n\nwrite_csv(csv_cal, \"/Users/davidschoeman/Dropbox/Documents/USC_Admin/Calendar.csv\") # Write the output to a file\n\nLet’s have a peek at what that tibble looks like (bearing in mind that this is made up data):\n\ncsv_cal\n\n# A tibble: 10,751 × 3\n   Start               End                 Event                                \n   &lt;dttm&gt;              &lt;dttm&gt;              &lt;chr&gt;                                \n 1 2010-02-01 10:00:00 2010-02-01 11:00:00 BEnvSci Review Meet                  \n 2 2012-10-01 11:30:00 2012-10-01 13:00:00 meetings and IT software interests   \n 3 2012-10-02 12:00:00 2012-10-02 13:30:00 New Researchers Group Oct meeting    \n 4 2012-10-10 15:00:00 2012-10-10 17:00:00 Mobile Business Needs Workshop - Aca…\n 5 2012-10-16 12:00:00 2012-10-16 14:00:00 Future Fellowship Applicants Lunch w…\n 6 2012-11-06 09:00:00 2012-11-08 17:00:00 Re: Introductory R Workshop at USC   \n 7 2012-12-18 11:00:00 2012-12-18 12:00:00 EIF Follow Up Meeting                \n 8 2013-01-23 15:00:00 2013-01-23 15:15:00 Dave Shoeman                         \n 9 2013-02-25 15:00:00 2013-02-25 15:30:00 Fraser Island research               \n10 2013-02-26 12:00:00 2013-02-26 13:00:00 ClimateWatch marine science meeting  \n# ℹ 10,741 more rows\n\n\nOK, so now that we have the data, we can search for keywords and see how much time we’ve spent on those activities. We’ll first filter for a specific time period and make sure that our timezone is correctly set (this is important if you pull in the .csv we saved earlier):\n\nstrt_date &lt;- \"2024-01-01\" # A start date\nend_date &lt;- \"2024-12-31\" # An end date\n\ndat &lt;- csv_cal %&gt;% \n        filter(Start &gt;= strt_date & Start &lt;= end_date) %&gt;% # Filter by period of interest\n        mutate(Start = with_tz(Start, \"australia/brisbane\"), # Ensure that my timezone is set correctly\n                     End = with_tz(End, \"australia/brisbane\"), # And again\n                     Duration = (End - Start) %&gt;% \n                        as.numeric()/60/60) # Compute the duration of each event in hours\n\nNext, we group by event name, sum the time committed and remove any “activities” that might be placeholders rather than actual time commitments:\n\ntime_spent &lt;- dat %&gt;% \n        group_by(Event) %&gt;% \n        summarise(Time = sum(Duration)) %&gt;% # Compute the summed duration committed to each event\n        filter(str_detect(Event, \"WFH|Playing Wordle\", negate = TRUE)) # Remove any specific events that are not for logging time\n\nFinally, we write a short function to extract all lines containing a specific keyword, then use that function to look around:\n\nget_time &lt;- function(a) {\n            ts &lt;- time_spent %&gt;% \n                filter(str_detect(Event, a))\n            return(list(Time_table = data.frame(ts), # Output all rows containing the keyword, in case you need to check\n                         Total = ts %&gt;% \n                                        pull(Time) %&gt;% \n                                        sum())) # Also report total time spent on events containing that keyword\n            }\n\nOK, so let’s see how much time I have spent on my year-2 stats course so far this year:\n\nget_time(\"ANM203\")$Total\n\n[1] 239.75\n\n\nSooo…240 hours…and we still have four weeks of semester to go, and a major assignment to mark…oh well…\nOK, in the end, you can see that it’s not particularly difficult, but I hope this saves you some time if you ever want to analyse calendar data yourself."
  },
  {
    "objectID": "posts/2024-09-27_ics_to_csv/index.html",
    "href": "posts/2024-09-27_ics_to_csv/index.html",
    "title": "Converting a calendar export from .ics to .csv",
    "section": "",
    "text": "OK, so this is my first attempt at a blog. It came about because I had a discussion with my Dean about how much time I was spending on teaching vs research vs engagement. Being a data nerd, I record my activities in my Mac Calendar app as a matter of course, so I have a record of what I have been doing. But getting data out of a Calendar app and into a format you can analyse is more difficult. So I decided to write a script to get this done. And because most other options for doing this either cost money, or ask you to trust some or other skeevy website, I thought I’d share my code. Of course, sharing code also means that others can help me improve it, so let me know where I went wrong.\nFirst things first, the packages we’ll need:\n\nlibrary(tidyverse) # For data munging\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(parallel) # To count the cores on my machine, amongst other things\nlibrary(furrr) # To allow me to deploy a function in parallel\n\nLoading required package: future\n\n\nNext, we get the data — after I exported my calendar to a .ics file.\n\ndat &lt;- read_lines(\"/Users/davidschoeman/Dropbox/Documents/USC_Admin/Calendar_export_27092024.ics\") %&gt;% # It's a text file so read it as lines of text\n    tibble() %&gt;% # Make the result into a tibble\n    rename(Info = 1) # It'll have only one column; I'll call it Info\n\nNext, I figure out which rows indicate the start and end of each events, and I clump them together into a tibble:\n\nevents &lt;- tibble(Starts = dat %&gt;%\n                                    pull(Info) %&gt;% \n                                    str_which(\"BEGIN:VEVENT\"), # You might need to change the event-start identifier\n                                 Ends = dat %&gt;%\n                                    pull(Info) %&gt;% \n                                    str_which(\"END:VEVENT\")) # You might need to change the event-end identifier\n\nNext, we need a function to pull out the required details from each event:\n\nget_event_deets &lt;- function(s, e) {\n        d &lt;- dat[s:e,] # A row from dat, based on one row from events\n        get_bit &lt;- function(b) { # A function to look for a keyword and return the data\n            d %&gt;% \n                filter(str_detect(Info, b)) %&gt;% # Find the row in question \n                deframe() %&gt;% # Make it a vector\n                str_remove(b) %&gt;% # Remove the keyword\n                str_sub(2, -1) # Remove just the first character, which is either ; or :, leaving just the info to extract\n            }\n        return(list(Start = get_bit(\"DTSTART\") %&gt;% \n                                str_remove(\"TZID=\") %&gt;% # Remove the time identifier, if present\n                                str_split(\":\") %&gt;% # Split on the colon\n                                unlist() %&gt;% # Make it a vector\n                                nth(2) %&gt;% # Select second element\n                                strptime(., \"%Y%m%dT%H%M%OS\", tz = \"australia/brisbane\"), # Make it time\n                            End = get_bit(\"DTEND\") %&gt;% \n                                str_remove(\"TZID=\") %&gt;% \n                                str_split(\":\") %&gt;% \n                                unlist() %&gt;% \n                                nth(2) %&gt;% \n                                strptime(., \"%Y%m%dT%H%M%OS\", tz = \"australia/brisbane\"), \n                            Event = get_bit(\"SUMMARY\")))\n        }\n\nFinally, we deploy the function against the events tibble…in parallel, because it’s quite large. If you prefer to run on a single core, hash out the lines starting with plan(), and delete future_ from the line with future_pmap:\n\nplan(multisession, workers = detectCores()-4) # Using all but four of my cores\n    csv_cal &lt;- future_pmap(events, # For each row of events\n                           ~get_event_deets(..1, ..2)) %&gt;%  # Get the details\n      bind_rows() %&gt;% # Bind output lists into a tibble\n      arrange(Start) # Place in sequential order\n    plan(sequential)\n\nFinally, we save the output:\n\nwrite_csv(csv_cal, \"/Users/davidschoeman/Dropbox/Documents/USC_Admin/Calendar.csv\") # Write the output to a file\n\nLet’s have a peek at what that tibble looks like (bearing in mind that this is made up data):\n\ncsv_cal\n\n# A tibble: 10,751 × 3\n   Start               End                 Event                                \n   &lt;dttm&gt;              &lt;dttm&gt;              &lt;chr&gt;                                \n 1 2010-02-01 10:00:00 2010-02-01 11:00:00 BEnvSci Review Meet                  \n 2 2012-10-01 11:30:00 2012-10-01 13:00:00 meetings and IT software interests   \n 3 2012-10-02 12:00:00 2012-10-02 13:30:00 New Researchers Group Oct meeting    \n 4 2012-10-10 15:00:00 2012-10-10 17:00:00 Mobile Business Needs Workshop - Aca…\n 5 2012-10-16 12:00:00 2012-10-16 14:00:00 Future Fellowship Applicants Lunch w…\n 6 2012-11-06 09:00:00 2012-11-08 17:00:00 Re: Introductory R Workshop at USC   \n 7 2012-12-18 11:00:00 2012-12-18 12:00:00 EIF Follow Up Meeting                \n 8 2013-01-23 15:00:00 2013-01-23 15:15:00 Dave Shoeman                         \n 9 2013-02-25 15:00:00 2013-02-25 15:30:00 Fraser Island research               \n10 2013-02-26 12:00:00 2013-02-26 13:00:00 ClimateWatch marine science meeting  \n# ℹ 10,741 more rows\n\n\nOK, so now that we have the data, we can search for keywords and see how much time we’ve spent on those activities. We’ll first filter for a specific time period and make sure that our timezone is correctly set (this is important if you pull in the .csv we saved earlier):\n\nstrt_date &lt;- \"2024-01-01\" # A start date\nend_date &lt;- \"2024-12-31\" # An end date\n\ndat &lt;- csv_cal %&gt;% \n        filter(Start &gt;= strt_date & Start &lt;= end_date) %&gt;% # Filter by period of interest\n        mutate(Start = with_tz(Start, \"australia/brisbane\"), # Ensure that my timezone is set correctly\n                     End = with_tz(End, \"australia/brisbane\"), # And again\n                     Duration = (End - Start) %&gt;% \n                        as.numeric()/60/60) # Compute the duration of each event in hours\n\nNext, we group by event name, sum the time committed and remove any “activities” that might be placeholders rather than actual time commitments:\n\ntime_spent &lt;- dat %&gt;% \n        group_by(Event) %&gt;% \n        summarise(Time = sum(Duration)) %&gt;% # Compute the summed duration committed to each event\n        filter(str_detect(Event, \"WFH|Playing Wordle\", negate = TRUE)) # Remove any specific events that are not for logging time\n\nFinally, we write a short function to extract all lines containing a specific keyword, then use that function to look around:\n\nget_time &lt;- function(a) {\n            ts &lt;- time_spent %&gt;% \n                filter(str_detect(Event, a))\n            return(list(Time_table = data.frame(ts), # Output all rows containing the keyword, in case you need to check\n                         Total = ts %&gt;% \n                                        pull(Time) %&gt;% \n                                        sum())) # Also report total time spent on events containing that keyword\n            }\n\nOK, so let’s see how much time I have spent on my year-2 stats course so far this year:\n\nget_time(\"ANM203\")$Total\n\n[1] 239.75\n\n\nSooo…240 hours…and we still have four weeks of semester to go, and a major assignment to mark…oh well…\nOK, in the end, you can see that it’s not particularly difficult, but I hope this saves you some time if you ever want to analyse calendar data yourself."
  },
  {
    "objectID": "posts/2024-10-10_A_function_with_data_masking/index.html",
    "href": "posts/2024-10-10_A_function_with_data_masking/index.html",
    "title": "Explorations in functional programming in R",
    "section": "",
    "text": "When students show the initiative to use R beyond just ANM203, I sometimes try to show them tips and tricks that will make their journey into programming a little easier. Often, this involves showing them how to build simple functions.\nIn this case, I thought I could build a simple function to compute Simpson’s Diversity index in a simple group_by() — summarise() workflow.\nBoy, was I wrong.\nThe answer turns out to be tricky, but not that tricky."
  },
  {
    "objectID": "posts/2024-10-10_A_function_with_data_masking/index.html#the-background",
    "href": "posts/2024-10-10_A_function_with_data_masking/index.html#the-background",
    "title": "Explorations in functional programming in R",
    "section": "",
    "text": "When students show the initiative to use R beyond just ANM203, I sometimes try to show them tips and tricks that will make their journey into programming a little easier. Often, this involves showing them how to build simple functions.\nIn this case, I thought I could build a simple function to compute Simpson’s Diversity index in a simple group_by() — summarise() workflow.\nBoy, was I wrong.\nThe answer turns out to be tricky, but not that tricky."
  },
  {
    "objectID": "posts/2024-10-10_A_function_with_data_masking/index.html#building-the-function",
    "href": "posts/2024-10-10_A_function_with_data_masking/index.html#building-the-function",
    "title": "Explorations in functional programming in R",
    "section": "Building the function",
    "text": "Building the function\nReading the data\nThe input data were saved as an .rds file, so we will attach the tidyverse, then read in the data:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ndat &lt;- read_rds(\"/Users/davidschoeman/Downloads/data.rds\") # Of course, if your data were in a .csv, you'd read_csv()\nhead(dat)\n\n# A tibble: 6 × 6\n  Transect_code Campus Common_Name           Scientific_Name  Observations Year \n  &lt;fct&gt;         &lt;fct&gt;  &lt;fct&gt;                 &lt;fct&gt;                   &lt;dbl&gt; &lt;fct&gt;\n1 SDL2          SD     Brown thornbill       Acanthiza pusil…            1 2024 \n2 MBR           MB     Buff rumped thornbill Acanthiza regul…            1 2022 \n3 MBR           MB     Eastern Spinebill     Acanthorhynchus…            1 2023 \n4 MBP1          MB     Eastern Spinebill     Acanthorhynchus…            1 2021 \n5 MBR           MB     Eastern Spinebill     Acanthorhynchus…            2 2021 \n6 MBR           MB     Eastern Spinebill     Acanthorhynchus…            4 2021 \n\n\nA workflow for computing Simpson’s Index\nThe idea is to build a function to compute Simpson’s index using Specific_Name to identify the species involved, and Observations to represent their abundances.\nBefore we explore this as a workflow, we need to know the formula for the Simpson Diversity Index, D:\n\\[\nD = \\frac{\\Sigma_{i}n_{i}(n_{i}-1)}{N(N-1)},\n\\]\nwhere \\(n_{i}\\) is the abundance of species \\(i\\) and \\(N\\) is the sum of all species abundances (\\(\\Sigma_{i}n_{i}\\)).\nLet’s say that we wanted to compute this index for the SD Campus. The workflow would look something like this:\n\nd &lt;- dat %&gt;% \n  filter(Campus == \"SD\") # Filter for SD \n\nnumerator &lt;- d %&gt;% # First compute the numerator\n  group_by(Scientific_Name) %&gt;% # To be sure that there aren't duplicate lines per species\n  summarise(n = sum(Observations, na.rm = TRUE)) %&gt;% \n  mutate(n_min_1 = n - 1, # Add a variable for n-1\n         prod = n * n_min_1) %&gt;%  # The product of n and n-1\n  summarise(sum(prod, na.rm = TRUE)) %&gt;%  # The numerator is the sum of the product of n and n-1\n  deframe() # Make the output a number rather than a tibble\n\ndenominator &lt;- d %&gt;% # Next, the denominator\n  summarise(n = sum(Observations, na.rm = TRUE)) %&gt;% \n      mutate(n_min_1 = n - 1,\n             prod = n * n_min_1) %&gt;% # The demoniator is the sum of the total number of observations and the total minus one\n  pull(prod) # Extract just the product as a number\n\nS &lt;- numerator/denominator # Do the calculation\n\nprint(S) # OUtput the result\n\n[1] 0.1796166\n\n\nI was hoping to turn this into a function deployed with summarise(), but found that it isn’t straightforward to “feed” grouped data into a function this complex.\nSo, instead, we need to build a function that does the grouping as well as the summarising.\nBuilding the actual function\nThe first issue is that if we want to build a function that is going to group and summarise data, we need to allow that function to have arguments that are names of variables.\nAchieving this is somewhat trickier than it sounds because if you call variable name, R will invariably error out because that variable exists within a tibble/data.frame rather than being present in the environment.\nSo…we need to pretend that the variable name is not an object. We do this with a technique called “data masking”, which essentially means that you hide the variable name inside two pairs of braces: { var }, where var is the name of the variable.\nAnother issue is that if you want your function to be flexible as well as generic, it has to be able to take any number of arguments to specify grouping variables. This is done within the function call, by specifying open-ended arguments using the ellipsis: ...\nNote that to make a function, all we do is throw braces — {} — around a workflow, give the function a name and specify its arguments:\n\n  get_simpson &lt;- function(.data, spp, abund, ...) {\n    # .data: the short-hand for the data emerging from a tidyverse pipe (so no need to specify this argument when using the function... the dot means it is \"hidden\")\n    # spp: an argument for the variable containing the species identifiers\n    # abund: an argument for the name of the variable containing abundance by species\n    # ...: arguments for the names of the variables you want to group by\n    \n    numerator &lt;- .data %&gt;% # .data is the data passed by the pipe\n      group_by(..., {{ spp }}) %&gt;% # Group by named grouping variables, then by species identifier\n      summarise(n = sum({{ abund }}, na.rm = TRUE)) %&gt;% # {{ abund }} is the variable containing abundances\n        # Note that this step accounts for the possibility of repeat rows for a species for any combination of grouping variables\n        mutate(n_min_1 = n - 1, # Add a variable for n-1\n             prod = n * n_min_1) %&gt;% # Add a variable for the product of n and n-1\n      group_by(...) %&gt;% # Group the result by the grouping variables\n      summarise(numerator = sum(prod, na.rm = TRUE)) # Compute the sum of the products per group\n   \n    denominator &lt;- .data %&gt;%\n      group_by(...) %&gt;% # We want the denominator only for groups, NOT for the species\n      summarise(n = sum({{ abund }}, na.rm = TRUE)) %&gt;% # The overall sum per group\n      mutate(n_min_1 = n - 1, # n-1\n             denominator = n * n_min_1) %&gt;% # n * n-1\n    select(-n, -n_min_1) # Remove unused rows\n    \n    out &lt;- left_join(numerator, denominator) %&gt;% # Combine the tibbles into a single tibble; R will use the grouping variables to ensure that values are in the correct rows\n      mutate(D = numerator/denominator) %&gt;% # Do the calculation per row/group\n      select(-numerator, -denominator) # Remove unwanted variables\n    \n  return(out) # Return the answer\n  } \n\nLet’s give it a whirl, knowing that the Simpson Diversity index for SD is 0.18:\n\ndat %&gt;% \n  get_simpson(Scientific_Name, Observations, Campus) # Spp ID, abundance, grouping variable\n\n`summarise()` has grouped output by 'Campus'. You can override using the\n`.groups` argument.\nJoining with `by = join_by(Campus)`\n\n\n# A tibble: 2 × 2\n  Campus      D\n  &lt;fct&gt;   &lt;dbl&gt;\n1 MB     0.0757\n2 SD     0.180 \n\n\nWorks! Or, at least, it gives the same answer as I got before.\nA more complex deployment, remembering that we can group by more than just campus:\n\ndat %&gt;% \n  get_simpson(Scientific_Name, Observations, Campus, Year) # Grouping by campus and year\n\n`summarise()` has grouped output by 'Campus', 'Year'. You can override using\nthe `.groups` argument.\n`summarise()` has grouped output by 'Campus'. You can override using the\n`.groups` argument.\n`summarise()` has grouped output by 'Campus'. You can override using the\n`.groups` argument.\nJoining with `by = join_by(Campus, Year)`\n\n\n# A tibble: 8 × 3\n# Groups:   Campus [2]\n  Campus Year       D\n  &lt;fct&gt;  &lt;fct&gt;  &lt;dbl&gt;\n1 MB     2021  0.0905\n2 MB     2022  0.0913\n3 MB     2023  0.0980\n4 MB     2024  0.0776\n5 SD     2021  0.132 \n6 SD     2022  0.361 \n7 SD     2023  0.0402\n8 SD     2024  0.0811\n\n\nThis seems to give rational outputs, although I would check before deploying in a serious context."
  },
  {
    "objectID": "posts/2024-10-10_A_function_with_data_masking/index.html#conclusion",
    "href": "posts/2024-10-10_A_function_with_data_masking/index.html#conclusion",
    "title": "Explorations in functional programming in R",
    "section": "Conclusion",
    "text": "Conclusion\nThis short blog is not about a diversity index, but rather about how easy it is to write and deploy fairly complex functions, allowing us to use variable names as unquoted arguments, and even to have an unlimited number of arguments in other contexts. Play around with these ideas…they have proven very helpful in my coding journey. I hope that they are equally useful to you."
  }
]